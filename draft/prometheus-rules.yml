apiVersion: "monitoring.coreos.com/v1"
kind: "PrometheusRule"
metadata:
  name: "prometheus-rules"
  namespace: "monitoring"
  labels:
    release: "prom-operator"
spec:
  groups:
    - name: "prometheus.rules"
      rules:
        - alert: "PrometheusBadConfig"
          expr: 'max_over_time(prometheus_config_last_reload_successful[5m]) == 0'
          for: "10m"
          labels:
            severity: "critical"
          annotations:
            summary: 'Failed Prometheus configuration reload.'
            description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed to reload its configuration.'
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig'

        - alert: "PrometheusSDRefreshFailure"
          expr: 'increase(prometheus_sd_refresh_failures_total[10m]) > 0'
          for: "20m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Failed Prometheus SD refresh.'
            description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed to refresh SD with mechanism {{ $labels.mechanism }}.'
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheussdrefreshfailure'

        - alert: "PrometheusNotificationQueueRunningFull"
          expr: |
            (predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > min_over_time(prometheus_notifications_queue_capacity[5m]))
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus alert notification queue predicted to run full in less than 30m.'
            description: 'Alert notification queue of Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is running full.'
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull'

        - alert: "PrometheusErrorSendingAlertsToSomeAlertmanagers"
          expr: |
            (rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])) * 100 > 1
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.'
            description: |
              {{ printf "%.1f" $value }}% errors while sending alerts from Prometheus {{ $labels.namespace }}/{{ $labels.pod }} to Alertmanager {{ $labels.alertmanager }}.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers'

        - alert: "PrometheusNotConnectedToAlertmanagers"
          expr: 'max_over_time(prometheus_notifications_alertmanagers_discovered[5m]) < 1'
          for: "10m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus is not connected to any Alertmanagers.'
            description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not connected to any Alertmanagers.'
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers'

        - alert: "PrometheusTSDBReloadsFailing"
          expr: 'increase(prometheus_tsdb_reloads_failures_total[3h]) > 0'
          for: "4h"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus has issues reloading blocks from disk.'
            description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has detected {{ $value | humanize }} reload failures over the last 3h.'
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing'

        - alert: "PrometheusTSDBCompactionsFailing"
          expr: 'increase(prometheus_tsdb_compactions_failed_total[3h]) > 0'
          for: "4h"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus has issues compacting blocks.'
            description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has detected {{ $value | humanize }} compaction failures over the last 3h.'
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing'

        - alert: "PrometheusNotIngestingSamples"
          expr: |
            (sum without(type) (rate(prometheus_tsdb_head_samples_appended_total[5m])) <= 0
            and (sum without(scrape_job) (prometheus_target_metadata_cache_entries) > 0
            or sum without(rule_group) (prometheus_rule_group_rules) > 0))
          for: "10m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus is not ingesting samples.'
            description: 'Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.'
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples'

        - alert: "PrometheusDuplicateTimestamps"
          expr: 'rate(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0'
          for: "10m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus is dropping samples with duplicate timestamps.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is dropping {{ printf "%.4g" $value }} samples/s with different values but duplicated timestamp.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps'

        - alert: "PrometheusOutOfOrderTimestamps"
          expr: 'rate(prometheus_target_scrapes_sample_out_of_order_total[5m]) > 0'
          for: "10m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus drops samples with out-of-order timestamps.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is dropping {{ printf "%.4g" $value }} samples/s with timestamps arriving out of order.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps'

        - alert: "PrometheusRemoteStorageFailures"
          expr: |
            (
                (
                    rate(prometheus_remote_storage_failed_samples_total[5m]) or rate(prometheus_remote_storage_samples_failed_total[5m])
                ) / (
                    (
                        rate(prometheus_remote_storage_failed_samples_total[5m]) or rate(prometheus_remote_storage_samples_failed_total[5m])
                    ) + (
                        rate(prometheus_remote_storage_succeeded_samples_total[5m]) or rate(prometheus_remote_storage_samples_total[5m])
                    )
                )
            ) * 100 > 1
          for: "15m"
          labels:
            severity: "critical"
          annotations:
            summary: 'Prometheus fails to send samples to remote storage.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} failed to send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name }}:{{ $labels.url }}
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures'

        - alert: "PrometheusRemoteWriteBehind"
          expr: |
            (
                max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds[5m]) - ignoring(remote_name,url)
                group_right max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds[5m])
            ) > 120
          for: "15m"
          labels:
            severity: "critical"
          annotations:
            summary: 'Prometheus remote write is behind.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} remote write is {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name }}:{{ $labels.url }}.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind'

        - alert: "PrometheusRemoteWriteDesiredShards"
          expr: |
            (max_over_time(prometheus_remote_storage_shards_desired[5m]) > max_over_time(prometheus_remote_storage_shards_max[5m]))
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: |
              Prometheus remote write desired shards calculation wants to run more than configured max shards.
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} remote write desired
              shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name }}:{{ $labels.url }},
              which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s"}` $labels.instance | query | first | value }}.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards'

        - alert: "PrometheusRuleFailures"
          expr: 'increase(prometheus_rule_evaluation_failures_total[5m]) > 0'
          for: "15m"
          labels:
            severity: "critical"
          annotations:
            summary: 'Prometheus is failing rule evaluations.'
            description: 'Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed to evaluate {{ printf "%.0f" $value }} rules in the last 5m.'
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusrulefailures'

        - alert: "PrometheusMissingRuleEvaluations"
          expr: 'increase(prometheus_rule_group_iterations_missed_total[5m]) > 0'
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus is missing rule evaluations due to slow rule group evaluation.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has missed {{ printf "%.0f" $value }} rule group evaluations in the last 5m.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations'

        - alert: "PrometheusTargetLimitHit"
          expr: 'increase(prometheus_target_scrape_pool_exceeded_target_limit_total[5m]) > 0'
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus has dropped targets because some scrape configs have exceeded the targets limit.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has dropped {{ printf "%.0f" $value }}
              targets because the number of targets exceeded the configured target_limit.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit'

        - alert: "PrometheusLabelLimitHit"
          expr: 'increase(prometheus_target_scrape_pool_exceeded_label_limits_total[5m]) > 0'
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus has dropped targets because some scrape configs have exceeded the labels limit.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has dropped {{ printf "%.0f" $value }}
              targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit'

        - alert: "PrometheusScrapeBodySizeLimitHit"
          expr: 'increase(prometheus_target_scrapes_exceeded_body_size_limit_total[5m]) > 0'
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus has dropped some targets that exceeded body size limit.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed {{ printf "%.0f" $value }}
              scrapes in the last 5m because some targets exceeded the configured body_size_limit.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapebodysizelimithit'

        - alert: "PrometheusScrapeSampleLimitHit"
          expr: 'increase(prometheus_target_scrapes_exceeded_sample_limit_total[5m]) > 0'
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus has failed scrapes that have exceeded the configured sample limit.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed {{ printf "%.0f" $value }}
              scrapes in the last 5m because some targets exceeded the configured sample_limit.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapesamplelimithit'

        - alert: "PrometheusTargetSyncFailure"
          expr: 'increase(prometheus_target_sync_failed_total[30m]) > 0'
          for: "5m"
          labels:
            severity: "critical"
          annotations:
            summary: 'Prometheus has failed to sync targets.'
            description: |
              {{ printf "%.0f" $value }} targets in Prometheus {{ $labels.namespace }}/{{ $labels.pod }} have failed to sync because invalid configuration was supplied.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure'

        - alert: "PrometheusHighQueryLoad"
          expr: |
            avg_over_time(prometheus_engine_queries[5m]) / max_over_time(prometheus_engine_queries_concurrent_max[5m]) > 0.8
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: 'Prometheus is reaching its maximum capacity serving concurrent requests.'
            description: |
              Prometheus {{ $labels.namespace }}/{{ $labels.pod }} query API has less than 20% available capacity in its query engine for the last 15 minutes.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheushighqueryload'

        - alert: "PrometheusErrorSendingAlertsToAnyAlertmanager"
          expr: |
            min without (alertmanager) (
                rate(prometheus_notifications_errors_total{alertmanager!~""}[5m])
                / rate(prometheus_notifications_sent_total{alertmanager!~""}[5m])
            ) * 100 > 3
          for: "15m"
          labels:
            severity: "critical"
          annotations:
            summary: 'Prometheus encounters more than 3% errors sending alerts to any Alertmanager.'
            description: |
              {{ printf "%.1f" $value }}% minimum errors while sending alerts from Prometheus {{ $labels.namespace }}/{{ $labels.pod }} to any Alertmanager.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager'

        - alert: "PrometheusTargetDown"
          expr: |
            100 * (count(up == 0) by (cluster, job, namespace, service) / count(up) by (cluster, job, namespace, service)) > 10
          for: "10m"
          labels:
            severity: "warning"
          annotations:
            summary: 'One or more targets are unreachable.'
            description: |
              {{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.
            runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/general/targetdown'
