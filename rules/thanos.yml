# https://monitoring.mixins.dev/thanos/
apiVersion: "monitoring.coreos.com/v1"
kind: "PrometheusRule"
metadata:
  name: "thanos-rules"
  namespace: "monitoring"
  labels:
    release: "prom-operator"
spec:
  groups:
    - name: "thanos-record.rules"
      rules:
        - expr: |
            (
              sum by(job) (rate(grpc_client_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-query.*", grpc_type="unary"}[5m]))
            /
              sum by(job) (rate(grpc_client_started_total{job=~".*thanos-query.*", grpc_type="unary"}[5m]))
            )
          record: ":grpc_client_failures_per_unary:sum_rate"

        - expr: |
            (
              sum by(job) (rate(grpc_client_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-query.*", grpc_type="server_stream"}[5m]))
            /
              sum by(job) (rate(grpc_client_started_total{job=~".*thanos-query.*", grpc_type="server_stream"}[5m]))
            )
          record: ":grpc_client_failures_per_stream:sum_rate"

        - expr: |
            (
              sum by(job) (rate(thanos_query_store_apis_dns_failures_total{job=~".*thanos-query.*"}[5m]))
            /
              sum by(job) (rate(thanos_query_store_apis_dns_lookups_total{job=~".*thanos-query.*"}[5m]))
            )
          record: ":thanos_query_store_apis_dns_failures_per_lookup:sum_rate"

        - expr: 'histogram_quantile(0.99, sum by(job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query"}[5m])))'
          record: ":query_duration_seconds:histogram_quantile"
          labels:
            quantile: "0.99"

        - expr: 'histogram_quantile(0.99, sum by(job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query_range"}[5m])))'
          record: ":api_range_query_duration_seconds:histogram_quantile"
          labels:
            quantile: "0.99"

        - expr: |
            (
              sum by(job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-receive.*", grpc_type="unary"}[5m]))
            /
              sum by(job) (rate(grpc_server_started_total{job=~".*thanos-receive.*", grpc_type="unary"}[5m]))
            )
          record: ":grpc_server_failures_per_unary:sum_rate"

        - expr: |
            (
              sum by(job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-receive.*", grpc_type="server_stream"}[5m]))
            /
              sum by(job) (rate(grpc_server_started_total{job=~".*thanos-receive.*", grpc_type="server_stream"}[5m]))
            )
          record: ":grpc_server_failures_per_stream:sum_rate"

        - expr: |
            (
              sum by(job) (rate(http_requests_total{handler="receive", job=~".*thanos-receive.*", code!~"5.."}[5m]))
            /
              sum by(job) (rate(http_requests_total{handler="receive", job=~".*thanos-receive.*"}[5m]))
            )
          record: ":http_failure_per_request:sum_rate"

        - expr: 'histogram_quantile(0.99, sum by(job, le) (rate(http_request_duration_seconds_bucket{handler="receive", job=~".*thanos-receive.*"}[5m])))'
          record: ":http_request_duration_seconds:histogram_quantile"
          labels:
            quantile: "0.99"

        - expr: |
            (
              sum by(job) (rate(thanos_receive_replications_total{result="error", job=~".*thanos-receive.*"}[5m]))
            /
              sum by(job) (rate(thanos_receive_replications_total{job=~".*thanos-receive.*"}[5m]))
            )
          record: ":thanos_receive_replication_failure_per_requests:sum_rate"

        - expr: |
            (
              sum by(job) (rate(thanos_receive_forward_requests_total{result="error", job=~".*thanos-receive.*"}[5m]))
            /
              sum by(job) (rate(thanos_receive_forward_requests_total{job=~".*thanos-receive.*"}[5m]))
            )
          record: ":thanos_receive_forward_failure_per_requests:sum_rate"

        - expr: |
            (
              sum by(job) (rate(thanos_receive_hashrings_file_errors_total{job=~".*thanos-receive.*"}[5m]))
            /
              sum by(job) (rate(thanos_receive_hashrings_file_refreshes_total{job=~".*thanos-receive.*"}[5m]))
            )
          record: ":thanos_receive_hashring_file_failure_per_refresh:sum_rate"

        - expr: |
            (
              sum by(job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-store.*", grpc_type="unary"}[5m]))
            /
              sum by(job) (rate(grpc_server_started_total{job=~".*thanos-store.*", grpc_type="unary"}[5m]))
            )
          record: ":grpc_server_failures_per_unary:sum_rate"

        - expr: |
            (
              sum by(job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-store.*", grpc_type="server_stream"}[5m]))
            /
              sum by(job) (rate(grpc_server_started_total{job=~".*thanos-store.*", grpc_type="server_stream"}[5m]))
            )
          record: ":grpc_server_failures_per_stream:sum_rate"

        - expr: |
            (
              sum by(job) (rate(thanos_objstore_bucket_operation_failures_total{job=~".*thanos-store.*"}[5m]))
            /
              sum by(job) (rate(thanos_objstore_bucket_operations_total{job=~".*thanos-store.*"}[5m]))
            )
          record: ":thanos_objstore_bucket_failures_per_operation:sum_rate"

        - expr: 'histogram_quantile(0.99, sum by(job, le) (rate(thanos_objstore_bucket_operation_duration_seconds_bucket{job=~".*thanos-store.*"}[5m])))'
          record: ":thanos_objstore_bucket_operation_duration_seconds:histogram_quantile"
          labels:
            quantile: "0.99"

    - name: "thanos-alert.rules"
      rules:
        - alert: "ThanosCompactMultipleRunning"
          expr: 'sum by(job) (up{job=~".*thanos-compact.*"}) > 1'
          for: "5m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Compact has multiple instances running."
            description: "No more than one Thanos Compact instance should be running at once. There are {{$value}} instances running."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanoscompactmultiplerunning"

        - alert: "ThanosCompactHalted"
          expr: 'thanos_compact_halted{job=~".*thanos-compact.*"} == 1'
          for: "5m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Compact has failed to run and is now halted."
            description: "Thanos Compact {{$labels.job}} has failed to run and now is halted."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanoscompacthalted"

        - alert: "ThanosCompactHighCompactionFailures"
          expr: |
            (
              sum by(job) (rate(thanos_compact_group_compactions_failures_total{job=~".*thanos-compact.*"}[5m]))
            /
              sum by(job) (rate(thanos_compact_group_compactions_total{job=~".*thanos-compact.*"}[5m])) * 100 > 5
            )
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Compact is failing to execute compactions."
            description: "Thanos Compact {{$labels.job}} is failing to execute {{$value | humanize}}% of compactions."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanoscompacthighcompactionfailures"

        - alert: "ThanosCompactBucketHighOperationFailures"
          expr: |
            (
              sum by(job) (rate(thanos_objstore_bucket_operation_failures_total{job=~".*thanos-compact.*"}[5m]))
            /
              sum by(job) (rate(thanos_objstore_bucket_operations_total{job=~".*thanos-compact.*"}[5m])) * 100 > 5
            )
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Compact Bucket is having a high number of operation failures."
            description: "Thanos Compact {{$labels.job}} Bucket is failing to execute {{$value | humanize}}% of operations."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanoscompactbuckethighoperationfailures"

        - alert: "ThanosCompactHasNotRun"
          expr: '(time() - max by(job) (max_over_time(thanos_objstore_bucket_last_successful_upload_time{job=~".*thanos-compact.*"}[24h]))) / 60 / 60 > 24'
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Compact has not uploaded anything for last 24 hours."
            description: "Thanos Compact {{$labels.job}} has not uploaded anything for 24 hours."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanoscompacthasnotrun"

        - alert: "ThanosQueryHttpRequestQueryErrorRateHigh"
          expr: |
            (
              sum by(job) (rate(http_requests_total{code=~"5..", job=~".*thanos-query.*", handler="query"}[5m]))
            /
              sum by(job) (rate(http_requests_total{job=~".*thanos-query.*", handler="query"}[5m]))
            ) * 100 > 5
          for: "5m"
          labels:
            severity: "critical"
          annotations:
            summary: "Thanos Query is failing to handle requests."
            description: "Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}% of query requests."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryhttprequestqueryerrorratehigh"

        - alert: "ThanosQueryHttpRequestQueryRangeErrorRateHigh"
          expr: |
            (
              sum by(job) (rate(http_requests_total{code=~"5..", job=~".*thanos-query.*", handler="query_range"}[5m]))
            /
              sum by(job) (rate(http_requests_total{job=~".*thanos-query.*", handler="query_range"}[5m]))
            ) * 100 > 5
          for: "5m"
          labels:
            severity: "critical"
          annotations:
            summary: "Thanos Query is failing to handle requests."
            description: "Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}% of query_range requests."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryhttprequestqueryrangeerrorratehigh"

        - alert: "ThanosQueryGrpcServerErrorRate"
          expr: |
            (
              sum by(job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-query.*"}[5m]))
            /
              sum by(job) (rate(grpc_server_started_total{job=~".*thanos-query.*"}[5m])) * 100 > 5
            )
          for: "5m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Query is failing to handle requests."
            description: "Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}% of requests."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosquerygrpcservererrorrate"

        - alert: "ThanosQueryGrpcClientErrorRate"
          expr: |
            (
              sum by(job) (rate(grpc_client_handled_total{grpc_code!="OK", job=~".*thanos-query.*"}[5m]))
            /
              sum by(job) (rate(grpc_client_started_total{job=~".*thanos-query.*"}[5m]))
            ) * 100 > 5
          for: "5m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Query is failing to send requests."
            description: "Thanos Query {{$labels.job}} is failing to send {{$value | humanize}}% of requests."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosquerygrpcclienterrorrate"

        - alert: "ThanosQueryHighDNSFailures"
          expr: |
            (
              sum by(job) (rate(thanos_query_store_apis_dns_failures_total{job=~".*thanos-query.*"}[5m]))
            /
              sum by(job) (rate(thanos_query_store_apis_dns_lookups_total{job=~".*thanos-query.*"}[5m]))
            ) * 100 > 1
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Query is having high number of DNS failures."
            description: "Thanos Query {{$labels.job}} have {{$value | humanize}}% of failing DNS queries for store endpoints."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryhighdnsfailures"

        - alert: "ThanosQueryInstantLatencyHigh"
          expr: |
            (
              histogram_quantile(0.99, sum by(job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query"}[5m]))) > 40
            and
              sum by(job) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query"}[5m])) > 0
            )
          for: "10m"
          labels:
            severity: "critical"
          annotations:
            summary: "Thanos Query has high latency for queries."
            description: "Thanos Query {{$labels.job}} has a 99th percentile latency of {{$value}} seconds for instant queries."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryinstantlatencyhigh"

        - alert: "ThanosQueryRangeLatencyHigh"
          expr: |
            (
              histogram_quantile(0.99, sum by(job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query_range"}[5m]))) > 90
            and
              sum by(job) (rate(http_request_duration_seconds_count{job=~".*thanos-query.*", handler="query_range"}[5m])) > 0
            )
          for: "10m"
          labels:
            severity: "critical"
          annotations:
            summary: "Thanos Query has high latency for queries."
            description: "Thanos Query {{$labels.job}} has a 99th percentile latency of {{$value}} seconds for range queries."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryrangelatencyhigh"

        - alert: "ThanosQueryOverload"
          expr: |
            (
              max_over_time(thanos_query_concurrent_gate_queries_max[5m]) - avg_over_time(thanos_query_concurrent_gate_queries_in_flight[5m]) < 1
            )
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos query reaches its maximum capacity serving concurrent requests."
            description: |
              Thanos Query {{$labels.job}} has been overloaded for more than
              15 minutes. This may be a symptom of excessive simultanous complex requests,
              low performance of the Prometheus API, or failures within these components.
              Assess the health of the Thanos query instances, the connnected Prometheus
              instances, look for potential senders of these requests and then contact support.
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryoverload"

        - alert: "ThanosReceiveHttpRequestErrorRateHigh"
          expr: |
            (
              sum by(job) (rate(http_requests_total{code=~"5..", job=~".*thanos-receive.*", handler="receive"}[5m]))
            /
              sum by(job) (rate(http_requests_total{job=~".*thanos-receive.*", handler="receive"}[5m]))
            ) * 100 > 5
          for: "5m"
          labels:
            severity: "critical"
          annotations:
            summary: "Thanos Receive is failing to handle requests."
            description: "Thanos Receive {{$labels.job}} is failing to handle {{$value | humanize}}% of requests."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceivehttprequesterrorratehigh"

        - alert: "ThanosReceiveHttpRequestLatencyHigh"
          expr: |
            (
              histogram_quantile(0.99, sum by(job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-receive.*", handler="receive"}[5m]))) > 10
            and
              sum by(job) (rate(http_request_duration_seconds_count{job=~".*thanos-receive.*", handler="receive"}[5m])) > 0
            )
          for: "10m"
          labels:
            severity: "critical"
          annotations:
            summary: "Thanos Receive has high HTTP requests latency."
            description: "Thanos Receive {{$labels.job}} has a 99th percentile latency of {{ $value }} seconds for requests."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceivehttprequestlatencyhigh"

        - alert: "ThanosReceiveHighReplicationFailures"
          expr: |
            thanos_receive_replication_factor > 1 and (
              (
                sum by(job) (rate(thanos_receive_replications_total{result="error", job=~".*thanos-receive.*"}[5m]))
              /
                sum by(job) (rate(thanos_receive_replications_total{job=~".*thanos-receive.*"}[5m]))
              ) > (
                max by(job) (floor((thanos_receive_replication_factor{job=~".*thanos-receive.*"}+1) / 2))
              /
                max by(job) (thanos_receive_hashring_nodes{job=~".*thanos-receive.*"})
              )
            ) * 100
          for: "5m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Receive is having high number of replication failures."
            description: "Thanos Receive {{$labels.job}} is failing to replicate {{$value | humanize}}% of requests."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceivehighreplicationfailures"

        - alert: "ThanosReceiveHighForwardRequestFailures"
          expr: |
            (
              sum by(job) (rate(thanos_receive_forward_requests_total{result="error", job=~".*thanos-receive.*"}[5m]))
            /
              sum by(job) (rate(thanos_receive_forward_requests_total{job=~".*thanos-receive.*"}[5m]))
            ) * 100 > 20
          for: "5m"
          labels:
            severity: "info"
          annotations:
            summary: "Thanos Receive is failing to forward requests."
            description: "Thanos Receive {{$labels.job}} is failing to forward {{$value | humanize}}% of requests."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceivehighforwardrequestfailures"

        - alert: "ThanosReceiveHighHashringFileRefreshFailures"
          expr: |
            (
              sum by(job) (rate(thanos_receive_hashrings_file_errors_total{job=~".*thanos-receive.*"}[5m]))
            /
              sum by(job) (rate(thanos_receive_hashrings_file_refreshes_total{job=~".*thanos-receive.*"}[5m])) > 0
            )
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Receive is failing to refresh hasring file."
            description: "Thanos Receive {{$labels.job}} is failing to refresh hashring file, {{$value | humanize}} of attempts failed."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceivehighhashringfilerefreshfailures"

        - alert: "ThanosReceiveConfigReloadFailure"
          expr: 'avg by(job) (thanos_receive_config_last_reload_successful{job=~".*thanos-receive.*"}) != 1'
          for: "5m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Receive has not been able to reload configuration."
            description: "Thanos Receive {{$labels.job}} has not been able to reload hashring configurations."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceiveconfigreloadfailure"

        - alert: "ThanosReceiveNoUpload"
          expr: |
            (up{job=~".*thanos-receive.*"} - 1)
            + on(job, instance) # filters to only alert on current instance last 3h
            (sum by(job, instance) (increase(thanos_shipper_uploads_total{job=~".*thanos-receive.*"}[3h])) == 0)
          for: "3h"
          labels:
            severity: "critical"
          annotations:
            summary: "Thanos Receive has not uploaded latest data to object storage."
            description: "Thanos Receive {{$labels.instance}} has not uploaded latest data to object storage."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceivenoupload"

        - alert: "ThanosReceiveLimitsConfigReloadFailure"
          expr: 'sum by(job) (increase(thanos_receive_limits_config_reload_err_total{job=~".*thanos-receive.*"}[5m])) > 0'
          for: "5m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Receive has not been able to reload the limits configuration."
            description: "Thanos Receive {{$labels.job}} has not been able to reload the limits configuration."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceivelimitsconfigreloadfailure"

        - alert: "ThanosReceiveLimitsHighMetaMonitoringQueriesFailureRate"
          expr: '(sum by(job) (increase(thanos_receive_metamonitoring_failed_queries_total{job=~".*thanos-receive.*"}[5m])) / 20) * 100 > 20'
          for: "5m"
          labels:
            severity: "warning"
          annotations:
            summary: "Thanos Receive has not been able to update the number of head series."
            description: "Thanos Receive {{$labels.job}} is failing for {{$value | humanize}}% of meta monitoring queries."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceivelimitshighmetamonitoringqueriesfailurerate"

        - alert: "ThanosReceiveTenantLimitedByHeadSeries"
          expr: 'sum by(job, tenant) (increase(thanos_receive_head_series_limited_requests_total{job=~".*thanos-receive.*"}[5m])) > 0'
          for: "5m"
          labels:
            severity: "warning"
          annotations:
            summary: "A Thanos Receive tenant is limited by head series."
            description: "Thanos Receive tenant {{$labels.tenant}} is limited by head series."
            runbook_url: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosreceivetenantlimitedbyheadseries"
