apiVersion: "monitoring.coreos.com/v1"
kind: "PrometheusRule"
metadata:
  name: "loki-rules"
  namespace: "monitoring"
  labels:
    release: "prom-operator"
spec:
  groups:
    - name: "loki-alert.rules"
      rules:
        - alert: "LokiRequestErrors"
          expr: '100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[5m])) by(namespace, job, route) / sum(rate(loki_request_duration_seconds_count[5m])) by(namespace, job, route) > 10'
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: "Loki request errors (instance: {{ $labels.job }}/{{ $labels.route }})"
            description: "Loki request errors of {{ $labels.job }}/{{ $labels.route }} is {{ printf \"%.2f\" $value }}"
            runbook_url: ""

        - alert: "LokiRequestPanics"
          expr: 'sum(increase(loki_panic_total[15m])) by(namespace, job) > 0'
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: "Loki request panic (instance: {{ $labels.job }})"
            description: "Loki request panic of {{ $labels.job }} is {{ printf \"%.2f\" $value }}"
            runbook_url: ""

        - alert: "LokiRequestLatency"
          expr: '(histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m])) by(namespace, pod, route, le))) > 1'
          for: "15m"
          labels:
            severity: "warning"
          annotations:
            summary: "Loki request latency (instance: {{ $labels.job }}/{{ $labels.route }})"
            description: "Loki request latency of {{ $labels.job }}/{{ $labels.route }} is {{ printf \"%.2f\" $value }}s"
            runbook_url: ""
